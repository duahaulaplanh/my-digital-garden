---
{"dg-publish":true,"permalink":"/zettel/exercises-part-i/","noteIcon":"ğŸ“","created":"2024-04-23T09:04:07.504+07:00","updated":"2024-04-23T11:02:01.692+07:00"}
---

>[!example]+ Giáº£i bÃ i 1.1
>
>![Pasted image 20240423100559.png](/img/user/Attachment/Pasted%20image%2020240423100559.png)

PhÆ°Æ¡ng trÃ¬nh (1.1) nhÆ° sau:
$$
y(x, \mathbf{w}) = \sum_{j=0}^M x^j w_{j}
$$
PhÆ°Æ¡ng trÃ¬nh (1.2) nhÆ° sau:
$$
E(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \left\{ y(x_{n}, \mathbf{w}) - t_{n} \right\}^2
$$
vá»›i $(x_1, \dots, x_{n})$ lÃ  cÃ¡c dá»¯ liá»‡u, $(t_{1}, \dots, t_{n})$ lÃ  cÃ¡c nhÃ£n (hay lÃ  target) vÃ  $\mathbf{w}$ lÃ  bá»™ tham sá»‘. Äá»ƒ cá»±c tiá»ƒu Ä‘Æ°á»£c hÃ m lá»—i $E(\mathbf{w})$, ta chá»‰ cáº§n tÃ¬m giÃ¡ trá»‹ $\hat{\mathbf{w}} = (\hat{w}_{1}, \dots, \hat{w}_{M})^T$, sao cho:
$$
\nabla E(\hat{\mathbf{w}}) = \mathbf{0}
$$
vá»›i $\mathbf{0}$ lÃ  vector $0$. Do $\nabla E(\mathbf{w})$ lÃ  má»™t vector nÃªn Ä‘á»ƒ vector báº±ng $0$ thÃ¬ tá»«ng pháº§n tá»­ báº±ng $0$, váº­y ta cáº§n tÃ¬m $\hat{w}_{i}$ sao cho:
$$
\frac{\partial{E(\hat{\mathbf{w}})}}{\partial w_{i}} = 0
$$
Giá» báº¯t Ä‘áº§u biáº¿n Ä‘á»•i Ä‘á»ƒ tÃ¬m Ä‘Æ°á»£c $\hat{w}_i$ thÃ´i nÃ o. Ta cÃ³:
$$
\begin{aligned}
\frac{\partial{E(\hat{\mathbf{w}})}}{\partial w_{i}} &= \frac{1}{2} \sum_{n=1}^N \frac{\partial \{y(x_{n}, \mathbf{w}) - t_{n}\}^2}{\partial w_{i}} \\
&= \frac{1}{2} \sum_{n=1}^N \left[ \frac{\partial y(x_{n}, \mathbf{w})^2}{\partial w_{i}} - 2t_{n} \frac{\partial y(x_{n}, \mathbf{w})}{\partial w_{i}} + \frac{\partial t_{n}^2}{\partial w_{i}} \right] \\
&= \frac{1}{2} \sum_{n=1}^N \left[ 2y(x_{n}, \mathbf{w})\frac{\partial y(x_{n}, \mathbf{w})}{\partial w_{i}} - 2t_{n}x_{n}^i +0 \right] \\
&= \frac{1}{2} \sum_{n=1}^N [2y(x_{n}, \mathbf{w})x_{n}^i -2t_{n}x_{n}^i] \\
&= \sum_{n=1}^N \left[ \sum_{j=0}^M x_{n}^{i+j} w_{j} \right] - \sum_{n=1}^Nt_{n}x_{n}^i
\end{aligned}
$$
á» phÆ°Æ¡ng trÃ¬nh trÃªn, tÃ¡ch pháº§n trÆ°á»›c dáº¥u trá»«, ta Ä‘Æ°á»£c:
$$
\begin{aligned}
\sum_{n=1}^N \left[ \sum_{j=0}^M x_{n}^{i+j} w_{j} \right] &= \sum_{n=1}^N (x_{n}^{i}w_{0} + \dots x_{n}^{i+M}w_{M}) \\
&= \sum_{n=1}^N x_{n}^iw_{0} + \dots + \sum_{n=1}^N x_{n}^{i+M}w_{M} \\
&= \sum_{j=0}^M \left( \sum_{n=1}^N x_{n}^{i+j} \right) w_{j}
\end{aligned}
$$
Thay vÃ o láº¡i phÆ°Æ¡ng trÃ¬nh chá»— ta cÃ³:
$$
\begin{aligned}
\frac{\partial{E(\hat{\mathbf{w}})}}{\partial w_{i}} &= \sum_{n=1}^N \left[ \sum_{j=0}^M x_{n}^{i+j} w_{j} \right] - \sum_{n=1}^Nt_{n}x_{n}^i \\
&= \sum_{j=0}^M \left( \sum_{n=1}^N x_{n}^{i+j} \right) w_{j} - \sum_{n=1}^N t_{n}x_{n}^i
\end{aligned}
$$
Äáº·t $\sum_{n=1}^N x_{n}^{i+j} = A_{ij}$ vÃ  $\sum_{n=1}^Nt_{n}x_{n}^i = T_{i}$. Khi Ä‘Ã³:
$$
\begin{aligned}
\frac{\partial E(\mathbf{w})}{\partial w_{i}} &= 0 \\
\Leftrightarrow \sum_{j=0}^M A_{ij}w_{j} &= T_{i}
\end{aligned}
$$
Váº­y giÃ¡ trá»‹ $\hat{w}_i$ cáº§n tÃ¬m chÃ­nh lÃ  nghiá»‡m cá»§a phÆ°Æ¡ng trÃ¬nh trÃªn.

>[!example]+ Giáº£i bÃ i 1.2
>
>![Pasted image 20240423100741.png](/img/user/Attachment/Pasted%20image%2020240423100741.png)

PhÆ°Æ¡ng trÃ¬nh (1.122) lÃ  phÆ°Æ¡ng trÃ¬nh nghiá»‡m $\hat{w}_i$ á»Ÿ bÃ i 1.1. CÃ²n phÆ°Æ¡ng trÃ¬nh (1.4) nhÆ° sau:
$$
\tilde{E}(\mathbf{w}) = \frac{1}{2} \sum_{n=1}^N \{y(x_{n}, \mathbf{w}) - t_{n}\}^2 + \frac{\lambda}{2}||\mathbf{w||^2}
$$
trong Ä‘Ã³ $||\mathbf{w}||$ Ä‘Æ°á»£c gá»i lÃ  **chuáº©n** cá»§a vector $\mathbf{w}$ vÃ  cÃ³ cÃ´ng thá»©c lÃ :
$$
||\mathbf{w}|| = \sqrt{ w_{0}^2 + \dots + w_{M}^2 }
$$
Ta cÃ³:
$$
\begin{aligned}
\frac{\partial{E(\hat{\mathbf{w}})}}{\partial w_{i}} &= \frac{1}{2} \sum_{n=1}^N \frac{\partial \left( \{y(x_{n}, \mathbf{w}) - t_{n}\}^2 +\frac{\lambda}{2}||\mathbf{w}|| \right)}{\partial w_{i}} \\
&= \frac{1}{2} \sum_{n=1}^N \left[ \frac{\partial y(x_{n}, \mathbf{w})^2}{\partial w_{i}} - 2t_{n} \frac{\partial y(x_{n}, \mathbf{w})}{\partial w_{i}} + \frac{\partial t_{n}^2}{\partial w_{i}} + \frac{\lambda}{2} \frac{\partial||\mathbf{w}||^2}{\partial w_{i}} \right] \\
&= \sum_{j=0}^M A_{ij}w_{j} -T_{i} + \sum_{n=1}^N \lambda w_{i} \\
&= \sum_{j=0}^M A_{ij}w_{j} -T_{i} + N\lambda w_{i} \\
\end{aligned}
$$
Váº­y giÃ¡ trá»‹ $\hat{w}_i$ chÃ­nh lÃ  nghiá»‡m cá»§a phÆ°Æ¡ng trÃ¬nh:
$$
\sum_{j=0}^M A_{ij}w_{j} + N\lambda w_{i} = T_{i}
$$

>[!example]+ Giáº£i bÃ i 1.3
>
>![Pasted image 20240423103443.png](/img/user/Attachment/Pasted%20image%2020240423103443.png)

BÃ i nÃ y giáº£i khÃ¡ tÆ°Æ¡ng tá»± bÃ i trong pháº§n [[Zettel/Introduction (Prob)\|Introduction (Prob)]]. MÃ¬nh Ä‘áº·t biáº¿n ngáº«u nhiÃªn $B$ Ä‘áº¡i diá»‡n cho cÃ¡c há»™p mÃ u, biáº¿n ngáº«u nhiÃªn $F$ Ä‘áº¡i diá»‡n cho trÃ¡i cÃ¢y, $F= a$ lÃ  trÃ¡i tÃ¡o (apple), $F = o$ lÃ  trÃ¡i cam (orange) vÃ  $F = l$ lÃ  trÃ¡i chanh (lime). Ta cÃ³:
$$
\begin{aligned}
p(B = r) &= 0.2 \\
p(B = b) &= 0.2 \\
p(B = g) &= 0.6
\end{aligned}
$$
Tiáº¿p theo, ta cÃ³ xÃ¡c suáº¥t chá»n Ä‘Æ°á»£c trÃ¡i tÃ¡o khi Ä‘Ã£ chá»n Ä‘Æ°á»£c má»™t há»™p mÃ u láº§n lÆ°á»£t lÃ :
$$
\begin{aligned}
p(F = a \mid B = r) &= \frac{3}{10} = 0.3 \\
p(F = a \mid B = b) &= \frac{1}{2} = 0.5 \\
p(F = a \mid B = g) &= \frac{3}{10} = 0.3
\end{aligned}
$$
DÃ¹ng sum rule, ta láº¡i cÃ³:
$$
\begin{aligned}
p(F = a) &= \sum_{B} p(F=a, B) \\
&= \sum_{B} p(F=a \mid B)p(B) \\
&= p(F=a \mid B = r)p(B = r) + p(F=a \mid B=b)p(B=b) \\
&+ p(F=a \mid B=g)p(B = g) \\
&= 0.3 \times 0.2 + 0.5 \times 0.2 + 0.3 \times 0.6 \\
&= 0.34
\end{aligned}
$$
Váº­y xÃ¡c suáº¥t Ä‘á»ƒ chá»n Ä‘Æ°á»£c má»™t quáº£ tÃ¡o lÃ  $0.34$. Äá»ƒ tÃ¬m xÃ¡c suáº¥t há»™p ta chá»n lÃ  há»™p xanh lÃ¡ $g$ khi Ä‘Ã£ chá»n Ä‘Æ°á»£c quáº£ tÃ¡o, ta tÃ¬m xÃ¡c suáº¥t $p(B = g \mid F = a)$ báº±ng cÃ¡ch dÃ¹ng Ä‘á»‹nh lÃ½ Bayes:
$$
p(B = g \mid F= a) = \frac{p(F=a \mid B=g)p(B =g)}{p(F=a)} = \frac{0.3 \times 0.6}{0.34} \approx 0.52
$$

---

Pháº§n trÆ°á»›c: 
Pháº§n sau: 

---
# References

- [Bishop] Pattern Recoginition and Machine Learning (exercises of I.Introduction).
- [prml-web-sol.dvi (microsoft.com)](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/05/prml-web-sol-2009-09-08.pdf)