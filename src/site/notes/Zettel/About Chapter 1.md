---
{"dg-publish":true,"permalink":"/zettel/about-chapter-1/","noteIcon":"üìù","created":"2024-04-15T16:40:01.637+07:00","updated":"2024-04-17T15:08:03.056+07:00"}
---

## Ph√¢n ph·ªëi nh·ªã th·ª©c (binomial distribution)

M·ªôt ƒë·ªìng xu kh√¥ng ƒë·ªìng ch·∫•t (t·ª©c l√† x√°c su·∫•t ra m·∫∑t ng·ª≠a v√† m·∫∑t x·∫•p kh√¥ng ƒë·ªÅu nhau) c√≥ x√°c su·∫•t ra m·∫∑t ng·ª≠a l√† $f$. ƒê·∫∑t bi·∫øn ng·∫´u nhi√™n $X$ l√† s·ªë m·∫∑t ng·ª≠a khi tung ƒë·ªìng xu $N$ l·∫ßn, khi ƒë√≥, ph√¢n ph·ªëi x√°c su·∫•t c·ªßa $X$ s·∫Ω l√†:
$$
P(X = r \mid f, N) = P(r \mid f, N) = {N \choose r} f^r (1-f)^{N- r} 
$$
>[!note]+ Gi·∫£i th√≠ch
>Gi·∫£ s·ª≠ r·∫±ng m·ªói l·∫ßn tung ƒë·ªìng xu l√† ƒë·ªôc l·∫≠p v·ªõi nhau, ƒë·∫∑t bi·∫øn c·ªë l·∫ßn tung ƒë·∫ßu ti√™n l√† $A_1$, t∆∞∆°ng t·ª±, tung l·∫ßn th·ª© $i$ l√† $A_i$ v√† tung sau $N$ l√† $A$ v·∫≠y:
>$$
>P(A) = P(A_{1} \cap \dots \cap A_{n}) = P(A_{1})\dots P(A_{n})
>$$
>V·∫≠y n·∫øu tung $N$ l·∫ßn, trong ƒë√≥ $r$ l·∫ßn l√† m·∫∑t ng·ª≠a, t·ª©c l√† t·∫°i l·∫ßn $i$ n√†o ƒë√≥, ta tung ƒë∆∞·ª£c m·∫∑t ng·ª≠a, t·ª©c l√† $P(A_i) = f$, c√≥ $r$ l·∫ßn nh∆∞ v·∫≠y. Ngo√†i ra khi tung $N$ l·∫ßn m√† $r$ l·∫ßn m·∫∑t ng·ª≠a n√™n $N-r$ l·∫ßn c√≤n l·∫°i l√† m·∫∑t x·∫•p v·ªõi x√°c su·∫•t $1-f$. 
>$$
>(A) = f^r (1-f)^{N-r}
>$$
>N·∫øu ta xem m·ªói l·∫ßn tung th·ª© $i$ l√† m·ªôt "√¥" th·ª© $i$ trong $N$ √¥, th√¨ s·ªë c√°ch m√† tung $N$ l·∫ßn c√≥ $r$ m·∫∑t ng·ª≠a ta xem nh∆∞ s·ªë c√°ch ƒë·ªÉ ch·ªçn $r$ √¥ trong $N$ √¥, v·∫≠y s·ªë c√°ch s·∫Ω l√†:
>$$
>N \choose r
>$$
>Do $P(A)$ ch·ªâ l√† x√°c su·∫•t c·ªßa 1 c√°ch trong $N \choose r$ c√°ch, v√¨ v·∫≠y, x√°c su·∫•t ƒë·ªÉ m·∫∑t ng·ª≠a xu·∫•t hi·ªán $r$ l·∫ßn trong $N$ l·∫ßn tung l√†:
>$$
>{N \choose r} f^r (1-f)^{N-r}
>$$

Trung b√¨nh c·ªßa ph√¢n ph·ªëi tr√™n (ƒë∆∞·ª£c g·ªçi l√† **ph√¢n ph·ªëi nh·ªã th·ª©c**), k√≠ hi·ªáu l√† $\mathcal{E}[X]$, ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a nh∆∞ sau:
$$
\mathcal{E}[X] \equiv \sum_{r=0}^N P(X = r \mid f, N) r
$$
>[!note]+
>V·ªõi $n$ bi·∫øn ng·∫´u nhi√™n $X_1, X_2, ..., X_n$, trung b√¨nh c√≥ t√≠nh ch·∫•t sau:
>$$
>\mathcal{E}\left[ \sum_{i=1}^n X_i \right] = \sum_{i=1}^n \mathcal{E}[X_{i}]
>$$
>Ngo√†i ra, v·ªõi $\alpha \in \mathbb{R}$, ta c√≥:
>$$
\mathcal{E}[\alpha X_i] = \alpha \mathcal{E}[X_i]
>$$
>T√≠nh ch·∫•t cu·ªëi c√πng c·ªßa trung b√¨nh (ƒë∆∞·ª£c g·ªçi l√† [LOTUS](https://en.wikipedia.org/wiki/Law_of_the_unconscious_statistician)):
>$$
\mathcal{E}[g(X)] = \sum_{x} P(X = x)g(X)
>$$
>trong ƒë√≥ $P(x)$ l√† ph√¢n ph·ªëi x√°c su·∫•t c·ªßa $X$ v√† $\sum_{x}$ nghƒ©a l√† t·ªïng tr√™n to√†n b·ªô gi√° tr·ªã m√† $X$ c√≥.

N·∫øu ta xem $X_i$ l√† s·ªë m·∫∑t ng·ª≠a ·ªü l·∫ßn tung th·ª© $i$ (t·ª©c l√† t·∫°i l·∫ßn tung th·ª© $i$, ta ƒë∆∞·ª£c m·∫∑t ng·ª≠a hay m·∫∑t s·∫•p, ch·ªâ tung 1 l·∫ßn). V·∫≠y:
$$
X = X_1 + X_2 + \dots + X_N
$$
Ta bi·∫øt r·∫±ng, v·ªõi m·ªói l·∫ßn tung th·ª© $i$ th√¨ trung b√¨nh c·ªßa l·∫ßn tung ƒë√≥ l√† $\mathcal{E}[X_i]=\sum_{r=0}^1 P(X=r \mid f, 1)r = f\cdot 1 + (1-f) \cdot 0 = f$ . Khi ƒë√≥, trung b√¨nh c·ªßa ph√¢n ph·ªëi nh·ªã th·ª©c s·∫Ω l√†:
$$
\mathcal{E}[X] = \mathcal{E}[X_1 + X_2 + \dots + X_N] = \sum_{i=1}^N \mathcal{E}[X_i] = Nf
$$
Ta ƒë·ªãnh nghƒ©a **ph∆∞∆°ng sai** c·ªßa ph√¢n ph·ªëi nh·ªã th·ª©c, k√≠ hi·ªáu $\text{var}[X]$ l√†:
$$
\text{var}[X] \equiv \mathcal{E}[(X - \mathcal{E}[X])^2] = \mathcal{E}[X^2] - (\mathcal{E}[X])^2
$$
>[!note]+
>T∆∞∆°ng t·ª± v·ªõi trung b√¨nh, ph∆∞∆°ng sai c≈©ng c√≥ t√≠nh ch·∫•t:
>$$
>\text{var}\left[ \sum_{i=1}^n X_n \right] = \sum_{i=1}^n \text{var}[X_i]
>$$

N·∫øu ƒë·∫∑t $X_i$ t∆∞∆°ng t·ª± nh∆∞ trung b√¨nh, ta c√≥ ph∆∞∆°ng sai c·ªßa $X_i$ l√† 
$$
\begin{aligned}
\text{var}[X_i] &= \mathcal{E}[X_i^2] + (\mathcal{E}[X_i])^2 \\ 
&= \sum_{r=0}^1 P(X = r \mid f, 1)r^2 + (\mathcal{E}[X_i])^2 \\ 
&= f - f^2 = f(1-f)
\end{aligned}
$$
V·∫≠y, ph∆∞∆°ng sai c·ªßa ph√¢n ph·ªëi nh·ªã th·ª©c s·∫Ω l√†:
$$
\text{var}[X] = \text{var}[X_1 + \dots + X_N] = Nf(1-f)
$$
## X·∫•p x·ªâ $x!$ v√† $N \choose r$

X√©t ph√¢n ph·ªëi $X$ nh∆∞ d∆∞·ªõi ƒë√¢y:
$$
P(X = r \mid \lambda) = e^{-\lambda} \dfrac{\lambda^r}{r!} \hspace{5pt} \text{v·ªõi $r \in \{0, 1, 2, \dots\}$}.
$$
ta g·ªçi ph√¢n ph·ªëi tr√™n l√† **ph√¢n ph·ªëi Poisson**.

>[!note]+
>Ph√¢n ph·ªëi chu·∫©n hay c√≤n g·ªçi l√† ph√¢n ph·ªëi gaussian c√≥ c√¥ng th·ª©c nh∆∞ sau:
>$$
>P(X = x \mid \mu, \sigma) = \dfrac{1}{\sigma\sqrt{2 \pi}} e^{\dfrac{-(x - \mu)^2}{2\sigma^2}}.
>$$
>trong ƒë√≥ $\mu$ l√† trung b√¨nh c·ªßa $X$, $\sigma^2$ l√† ph∆∞∆°ng sai c·ªßa $X$ v√† $\sigma$ l√† ƒë·ªô l·ªách chu·∫©n c·ªßa $X$.

Khi $\lambda$ ƒë·ªß l·ªõn (th√¥ng th∆∞·ªùng l√† $\lambda > 1000$), ta c√≥ th·ªÉ x·∫•p x·ªâ ph√¢n ph·ªëi Poisson v·ªõi ph√¢n ph·ªëi chu·∫©n c√≥ $\mu = \lambda$ v√† $\sigma^2 = \lambda \implies \sigma = \sqrt{\lambda}$ (t√¨m hi·ªÉu th√™m ·ªü [ƒë√¢y](https://stats.stackexchange.com/questions/83283/normal-approximation-to-the-poisson-distribution?rq=1)), v·∫≠y:
$$
e^{-\lambda} \dfrac{\lambda^r}{r!} \simeq\dfrac{1}{\sqrt{2\pi \lambda}} e^{\dfrac{-(r - \lambda)^2}{2\lambda}}.
$$
X√©t ph∆∞∆°ng tr√¨nh x·∫•p x·ªâ ph√≠a tr√™n, n·∫øu thay $r = \lambda$, ta ƒë∆∞·ª£c:
$$
\begin{aligned}
e^{-\lambda} \dfrac{\lambda^\lambda}{\lambda!} &\simeq \dfrac{1}{\sqrt{2\pi \lambda}} e^{\dfrac{-(\lambda - \lambda)^2}{2\lambda}} \\
\Leftrightarrow e^{-\lambda} \dfrac{\lambda^\lambda}{\lambda!} &\simeq \dfrac{1}{\sqrt{2\pi \lambda}} \\
\Leftrightarrow \lambda! &\simeq \lambda^{\lambda} e^{-\lambda} \sqrt{2\pi\lambda}.
\end{aligned}
$$
·ªû ph∆∞∆°ng tr√¨nh x·∫•p x·ªâ tr√™n, ta c√≥ th·ªÉ th·∫•y giai th·ª´a c·ªßa m·ªôt s·ªë ƒë∆∞·ª£c x·∫•p x·ªâ b·∫±ng m·ªôt c√¥ng th·ª©c kh√°c, ta g·ªçi c√¥ng th·ª©c ƒë√≥ l√† **x·∫•p x·ªâ Stirling** ([Stirling's approximation](https://en.wikipedia.org/wiki/Stirling%27s_approximation)). N·∫øu ta l·∫•y $\ln$ c·ªßa x·∫•p x·ªâ stirling, ta ƒë∆∞·ª£c:
$$
\begin{aligned}
\ln x! &\simeq \ln x^xe^{-x}\sqrt{2\pi x} \\
&\simeq \ln x^x  + \ln e^{-x} + \ln \sqrt{ 2\pi x } \\
&\simeq x\ln x - x + \frac{1}{2}\ln 2 \pi x.
\end{aligned}
$$
>[!note]+
>Ngo√†i c√°ch vi·∫øt nh∆∞ tr√™n, ng∆∞·ªùi ta c≈©ng th∆∞·ªùng vi·∫øt nh∆∞ sau:
>$$
>x\ln x - x + O(\ln x).
>$$

Gi√° tr·ªã n·∫±m ph√≠a cu·ªëi $\frac{1}{2} \ln 2\pi x$ ƒë∆∞·ª£c g·ªçi l√† *next-order correction term* (m√¨nh c≈©ng kh√¥ng r√µ ƒëo·∫°n n√†y), ta ho√†n to√†n c√≥ th·ªÉ b·ªè n√≥ v√† s·ª≠ d·ª•ng:
$$
x! \simeq x\ln x - x.
$$
Ta bi·∫øt r·∫±ng:
$$
{N \choose r} = \frac{N!}{(N - r)! r!}.
$$
Do ƒë√≥, ta ho√†n to√†n c√≥ th·ªÉ x·∫•p x·ªâ $N \choose r$ b·∫±ng c√°ch d√πng x·∫•p x·ªâ Stirling:
$$
\begin{aligned}
\ln \frac{N!}{(N - r)! r!} &= \ln N! - \ln (N-r)! - \ln r \\
&\simeq N\ln N - N - (N-r)\ln(N-r) + (N-r) - r\ln r + r \\
&= N\ln N - (N-r)\ln(N-r) - r\ln r \\
&= (N-r)\ln N - (N-r)\ln(N-r) + r\ln N - r\ln r\\
&= (N-r)\ln \frac{N}{N-r} + r\ln \frac{N}{r}.
\end{aligned}
$$
Ta bi·∫øt r·∫±ng:
$$
\log_{2}x = \frac{\log_{e}x}{\log_{e}2} = \frac{\ln x}{\ln 2}.
$$
t·ª©c l√† gi·ªØa $\ln x$ v√† $\log_{2} x$ (n·∫øu ch·ªâ vi·∫øt $\log$ th√¨ ta hi·ªÉu ƒë√¢y l√† $\log_{2}$) ch·ªâ kh√°c nhau m·ªôt h·∫±ng s·ªë, do ƒë√≥ vi·ªác x·∫•p x·ªâ $\ln x!$ c√≥ th·ªÉ thay th·∫ø th√†nh $\log x!$ m√† kh√¥ng c·∫ßn thay ƒë·ªïi c√¥ng th·ª©c, ch·ªâ th√™m h·∫±ng s·ªë v√†o, do ƒë√≥ c√≥ th·ªÉ b·ªè lu√¥n h·∫±ng s·ªë ƒë√≥ trong khi x·∫•p x·ªâ.
$$
\log{N\choose r} \simeq (N-r)\log \frac{N}{N-r} + r\log \frac{N}{r}.
$$
ƒê·ªãnh nghƒ©a h√†m $H_2(x)$ nh∆∞ sau:
$$
H_{2}(x) \equiv x\log \frac{1}{x} + (1-x) \log \frac{1}{(1-x)}.
$$
ta g·ªçi $H_2(x)$ l√† **h√†m entropy nh·ªã ph√¢n** (binary entropy function).

D·ª±a v√†o h√†m $H_2(x)$, ta c√≥ th·ªÉ x·∫•p x·ªâ $\log{N \choose r}$ th√†nh:
$$
\log{N \choose r} \simeq N H_{2}(r / N).
$$
hay
$$
{N \choose r} = 2^{NH_{2}(r / N)}.
$$
---

Ph·∫ßn sau: [[Zettel/Probability, Entropy v√† Inference\|Probability, Entropy v√† Inference]]

---
# References

- [Deriving the Binomial PMF (dcgerard.github.io)](https://dcgerard.github.io/stat234/14_binomial_distribution.pdf)
- [MacKay] Information Theory, Inference and Learning Algorithms - MacKay (chapter 1)