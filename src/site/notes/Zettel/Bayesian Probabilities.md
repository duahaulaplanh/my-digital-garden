---
{"dg-publish":true,"permalink":"/zettel/bayesian-probabilities/","noteIcon":"üìù","created":"2024-04-22T12:19:56.445+07:00","updated":"2024-04-22T19:14:51.539+07:00"}
---

ƒê·ªãnh l√Ω Bayes c≈©ng c√≥ th·ªÉ √°p d·ª•ng trong Machine Leanring nh∆∞ sau. Gi·∫£ s·ª≠ r·∫±ng ta c√≥ t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ v√† m·ªôt model c√≥ b·ªô tham s·ªë l√† $\mathbf{w}$. M·ª•c ƒë√≠ch c·ªßa ta l√† v·ªõi t·∫≠p d·ªØ li·ªáu $\mathcal{D}$ nh∆∞ n√†y, b·ªô tham s·ªë $\mathbf{w}$ c·ªßa ta nh∆∞ n√†o m·ªõi l√† t·ªët, t·ª©c l√† t√¨m x√°c su·∫•t $p(\mathbf{w} \mid \mathcal{D})$.

Gi·∫£ s·ª≠ ta ƒë√£ ch·ªçn ƒë∆∞·ª£c m·ªôt m√¥ h√¨nh v·ªõi b·ªô tham s·ªë $\mathbf{w}$. Tr∆∞·ªõc khi c√≥ t·∫≠p d·ªØ li·ªáu $\mathcal{D}$, ta ng·∫ßm gi·∫£ ƒë·ªãnh r·∫±ng $\mathbf{w}$ c√≥ ph√¢n ph·ªëi l√† $p(\mathbf{w})$. S·ª≠ d·ª•ng ƒë·ªãnh l√Ω Bayes, ta c√≥:
$$
p(\mathbf{w} \mid \mathcal{D}) = \frac{p(\mathcal{D} \mid \mathbf{w})p(\mathbf{w})}{p(\mathcal{D})}
$$
Ph√¢n ph·ªëi $p(\mathcal{D} \mid \mathbf{w})$ ·ªü b√™n ph·∫£i c·ªßa ƒë·ªãnh l√Ω Bayes l√† m·ªôt h√†m ph·ª• thu·ªôc v√†o $\mathcal{D}$, th·∫ø nh∆∞ng n·∫øu ta xem ph√¢n ph·ªëi ·∫•y l√† m·ªôt h√†m ph·ª• thu·ªôc v√†o $\mathbf{w}$, th√¨ khi ƒë√≥ ta g·ªçi $p(\mathcal{D} \mid \mathbf{w})$ l√† **h√†m likelihood** (likelihood function).

>[!note]+
>Thay v√¨ vi·∫øt $p(\mathcal{D} \mid \mathbf{w})$ ƒë·ªÉ d·ªÖ b·ªã nh·∫ßm l·∫´n gi·ªØa ph√¢n ph·ªëi x√°c su·∫•t v√† h√†m likelihood, ta s·∫Ω k√≠ hi·ªáu:
>$$
\mathcal{L}(\mathbf{w} \mid \mathcal{D}) = p(\mathcal{D} \mid \mathbf{w})
>$$
>trong ƒë√≥ $\mathcal{L}(\mathbf{w} \mid \mathcal{D})$ l√† h√†m likelihood c√≥ bi·∫øn l√† $\mathbf{w}$ c√≤n $p(\mathcal{D} \mid \mathbf{w})$ l√† ph√¢n ph·ªëi c√≥ bi·∫øn l√† $\mathcal{D}$.

D·ª±a v√†o ƒë·ªãnh nghƒ©a c·ªßa likelihood, ta c√≥ th·ªÉ vi·∫øt l·∫°i ƒë·ªãnh l√Ω Bayes nh∆∞ sau:
$$
\text{posterior} \propto \text{likelihood} \times \text{prior}
$$
>[!note]+
>K√≠ hi·ªáu $\propto$ nghƒ©a l√† t·ªâ l·ªá. V√≠ d·ª•, chi·ªÅu cao ($h$) $= 1.2 \times$ c√¢n n·∫∑ng ($w$), l√∫c n√†y ta n√≥i chi·ªÅu cao t·ªâ l·ªá v·ªõi c√¢n n·∫∑ng hay $h \propto w$. ·ªû ƒë·ªãnh l√Ω Bayes, n·∫øu ta xem $1 / p(\mathcal{D})$ l√† m·ªôt h·∫±ng s·ªë (m√† n√≥ l√† m·ªôt h·∫±ng s·ªë th·∫≠t, b·ªüi v√¨ $\mathcal{D}$ kh√¥ng ƒë·ªïi r·ªìi) th√¨ $p(\mathbf{w} \mid \mathcal{D}) \propto \mathcal{L}(\mathbf{w} \mid \mathcal{D})p(\mathbf{w})$.

trong ƒë√≥ $\text{likelihood}, \text{posterior}$ v√† $\text{prior}$ ƒë·ªÅu l√† c√°c h√†m ph·ª• thu·ªôc v√†o $\mathbf{w}$. C√≤n gi√° tr·ªã $p(\mathcal{D})$ d∆∞·ªõi m·∫´u l√† m·ªôt h·∫±ng s·ªë, d√πng ƒë·ªÉ ƒë·∫£m b·∫£o r·∫±ng ph√¢n ph·ªëi h·∫≠u nghi·ªám ·ªü b√™n ph·∫£i ƒë·ªãnh l√Ω Bayes ƒë√∫ng l√† m·ªôt ph√¢n ph·ªëi (m·∫≠t ƒë·ªô x√°c su·∫•t).

>[!note]+
>√Åp d·ª•ng sum rule v√† product rule ([[Zettel/Probability Densities\|Probability Densities]]) cho bi·∫øn t·ª•c, ta c√≥:
>$$
>p(\mathcal{D}) = \int p(\mathcal{D} \mid \mathbf{w}) p(\mathbf{w}) d\mathbf{w}
>$$
>V·∫≠y:
>$$
\begin{aligned}
\int_{-\infty}^{\infty} p(\mathbf{w} \mid \mathcal{D}) d\mathbf{w} &= \int_{-\infty}^{\infty} \frac{p(\mathcal{D} \mid \mathbf{w})p(\mathbf{w})}{p(\mathcal{D})} d\mathbf{w} \\
&= \frac{\int p(\mathcal{D}\mid \mathbf{w})p(\mathbf{w}) d\mathbf{w}}{\int p(\mathcal{D}\mid \mathbf{w})p(\mathbf{w}) d\mathbf{w}} = 1
\end{aligned}
>$$
>C√≤n vi·ªác $p(\mathbf{w} \mid \mathcal{D}) \geq 0$ m√¨nh nghƒ© kh√° l√† d·ªÖ th·∫•y r·ªìi.

---

Ph·∫ßn tr∆∞·ªõc: [[Zettel/Expectations and covariances\|Expectations and covariances]]
Ph·∫ßn sau: [[Zettel/Gaussian Distribution\|Gaussian Distribution]]

---
# References

- [Bishop] Pattern Recognition and Machine Learning - Bishop (chapter 1.2)