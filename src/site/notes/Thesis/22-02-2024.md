---
{"dg-publish":true,"permalink":"/thesis/22-02-2024/","noteIcon":"üìù","created":"2024-03-20T00:13:20.824+07:00","updated":"2024-04-06T21:08:54.543+07:00"}
---

- **Terms** (ref: [[2304.14933] An Empirical Study of Multimodal Model Merging (arxiv.org)](https://arxiv.org/abs/2304.14933)):
	- Model merging via **interpolation**.
	- Model merging via [task arithmetic](https://arxiv.org/abs/2212.04089).
	- Fuses multiple models trained on different tasks to generate a multi-task solution.
	- **Previous studies**: trained on similar tasks and the same initialization.
	- **In this paper**: expand on this concept to a [multimodal](https://www.v7labs.com/blog/multimodal-deep-learning-guide) setup by merging **transformers** trained on different **modalities** (v√≠ d·ª• nh∆∞ text, vision, hear, ... ).
	- **Key factors impacting model performance**:
		- Initlization.
		- Merging mechanism.
		- Model architectures.
	- Model merging investigates the technique of merging two models trained on different tasks while preserving their original capabilities or even outperforming [multi-task training](https://arxiv.org/pdf/2009.09796.pdf).
	- This technique merging enables us to generate the multi-task solution without synchronous training and is especially useful when the models for merging are already trained and available online.
	- However, current literature has only applied model merging to models trained on similar tasks or even the same dataset
	- Many modality-specific models employ dedicated [transformer encoders](https://machinelearningmastery.com/the-transformer-model/) to encode vision/language inputs independently, and on top of which additional transformer layers are used for multimodal fusion.
	- We explore how to **leverage a well-trained modality-specific model** and **develop a modality-agnostic model from it via model merging**, with a goal of **achieving similar performance to the modality-agnostic baseline which is pre-trained from scratch**.

- **Terms** (ref: [[1910.05653] Model Fusion via Optimal Transport (arxiv.org)](https://arxiv.org/abs/1910.05653))
	- The most common approach is to **form an ensemble of models** and **average their individual predictions**, this approach is often rendered **infeasible by given resource constraints in terms of memory and computation**.
	- [Ensemble methods](https://machinelearningmastery.com/tour-of-ensemble-learning-algorithms/) have a very long history. They c**ombine the outputs of several different models** as a way to improve the prediction performance and robustness. However, **this requires maintaining the K trained models** and **running each of them at test time** (say, in order to average their outputs). 
	- **The simplest way** to fuse several parent networks into a single network of the same size is **direct weight averaging**; here for simplicity, we assume that all network architectures are identical.
	- A third way to combine two models is [distillation](https://arxiv.org/pdf/2106.02834.pdf), where one network is retrained on its training data, while jointly using the output predictions of the other ‚Äòteacher‚Äô network on those samples.