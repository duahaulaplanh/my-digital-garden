---
dg-publish: true
---

- **General**:
	- [Merge Large Language Models with mergekit (huggingface.co)](https://huggingface.co/blog/mlabonne/merge-models)
	- [Model Merge - (Frankenmerge) (matt-rickard.com)](https://matt-rickard.com/model-merge-frankenmerge)
- **Merging methods**:
	- Old: 
		- [[1910.05653] Model Fusion via Optimal Transport (arxiv.org)](https://arxiv.org/abs/1910.05653)
		- [[2203.05482] Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time (arxiv.org)](https://arxiv.org/abs/2203.05482)
		- [[2111.09832] Merging Models with Fisher-Weighted Averaging (arxiv.org)](https://arxiv.org/abs/2111.09832)
	- New:
		- [[2212.04089] Editing Models with Task Arithmetic (arxiv.org)](https://arxiv.org/abs/2212.04089) (Task arithmetic)
		- [Digitous/LLM-SLERP-Merge: Spherical Merge Pytorch/HF format Language Models with minimal feature loss. (github.com)](https://github.com/Digitous/LLM-SLERP-Merge) (SLERP)
		- [[2306.01708] TIES-Merging: Resolving Interference When Merging Models (arxiv.org)](https://arxiv.org/abs/2306.01708) (TIES)
		- [[2311.03099] Language Models are Super Mario: Absorbing Abilities from Homologous Models as a Free Lunch (arxiv.org)](https://arxiv.org/abs/2311.03099) (DARE)
		- [Paper page - Git Re-Basin: Merging Models modulo Permutation Symmetries (huggingface.co)](https://huggingface.co/papers/2209.04836) (Git Re-Basin)
		- [Paper page - Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models (huggingface.co)](https://huggingface.co/papers/2208.03306) (BTM)
		- [Paper page - ZipIt! Merging Models from Different Tasks without Training (huggingface.co)](https://huggingface.co/papers/2305.03053) (ZipIt)
	- Other:
		- [TehVenomm/LM_Transformers_BlockMerge: Image Diffusion block merging technique applied to transformers based Language Models. (github.com)](https://github.com/TehVenomm/LM_Transformers_BlockMerge/?tab=readme-ov-file)
	- After:
		- [Paper page - Fusing finetuned models for better pretraining (huggingface.co)](https://huggingface.co/papers/2204.03044)
		- [Paper page - On Cross-Layer Alignment for Model Fusion of Heterogeneous Neural Networks (huggingface.co)](https://huggingface.co/papers/2110.15538)
		- [Paper page - Resolving Interference When Merging Models (huggingface.co)](https://huggingface.co/papers/2306.01708)
- Why model merging works:
	- [Paper page - Model Merging by Uncertainty-Based Gradient Matching (huggingface.co)](https://huggingface.co/papers/2310.12808)
	- [(16) How (or why) does model merging work? : LocalLLaMA (reddit.com)](https://www.reddit.com/r/LocalLLaMA/comments/18x2vuj/how_or_why_does_model_merging_work/)
- Something Else:
	- [(16) Is it possible to merge transformers? [D] : MachineLearning (reddit.com)](https://www.reddit.com/r/MachineLearning/comments/122fj05/is_it_possible_to_merge_transformers_d/)
	- [(13) Dear Model Mergers, Have You Solved Merger of Different Model Families? : LocalLLaMA (reddit.com)](https://www.reddit.com/r/LocalLLaMA/comments/1867ddv/dear_model_mergers_have_you_solved_merger_of/)
